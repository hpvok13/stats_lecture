wandb: Currently logged in as: valmiki-kothare-vk. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.16.6 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /net/vast-storage.ib.cluster/scratch/user/valmiki/stats_lecture/wandb/run-20240410_015828-5qtgc03h
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run woven-frog-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/valmiki-kothare-vk/Price%20Predictor
wandb: üöÄ View run at https://wandb.ai/valmiki-kothare-vk/Price%20Predictor/runs/5qtgc03h
Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 0/67569 [00:00<?, ?it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 1/67569 [00:01<23:14:20,  1.24s/it]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 3/67569 [00:01<6:56:28,  2.70it/s] Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 5/67569 [00:01<3:59:41,  4.70it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 7/67569 [00:01<2:46:02,  6.78it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 9/67569 [00:01<2:10:13,  8.65it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 11/67569 [00:01<1:52:10, 10.04it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 13/67569 [00:02<1:39:49, 11.28it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 15/67569 [00:02<1:30:22, 12.46it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 17/67569 [00:02<1:24:28, 13.33it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 19/67569 [00:02<1:21:25, 13.83it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 21/67569 [00:02<1:19:02, 14.24it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 23/67569 [00:02<1:15:57, 14.82it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 25/67569 [00:02<1:13:25, 15.33it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 27/67569 [00:02<1:11:32, 15.73it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 29/67569 [00:03<1:11:22, 15.77it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 31/67569 [00:03<1:20:40, 13.95it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 33/67569 [00:03<1:16:39, 14.68it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 35/67569 [00:03<1:31:57, 12.24it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 37/67569 [00:03<1:24:10, 13.37it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 39/67569 [00:03<1:30:10, 12.48it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 41/67569 [00:03<1:23:04, 13.55it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 43/67569 [00:04<1:26:15, 13.05it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 45/67569 [00:04<1:20:19, 14.01it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 47/67569 [00:04<1:33:19, 12.06it/s]Training | Epoch: 0 | Loss: 0.0000 | Acc: 0.00%:   0%|          | 49/67569 [00:04<1:25:16, 13.20it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 49/67569 [00:04<1:25:16, 13.20it/s]  Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 51/67569 [00:04<1:31:14, 12.33it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 53/67569 [00:04<1:24:07, 13.38it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 55/67569 [00:05<1:26:51, 12.96it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 57/67569 [00:05<1:20:41, 13.94it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 59/67569 [00:05<1:31:36, 12.28it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 61/67569 [00:05<1:24:04, 13.38it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 63/67569 [00:05<1:31:45, 12.26it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 65/67569 [00:05<1:24:01, 13.39it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 67/67569 [00:06<1:32:09, 12.21it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 69/67569 [00:06<1:24:28, 13.32it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 71/67569 [00:06<1:32:37, 12.14it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 73/67569 [00:06<1:26:16, 13.04it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 75/67569 [00:06<1:22:11, 13.69it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 77/67569 [00:06<1:17:20, 14.54it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 79/67569 [00:06<1:25:38, 13.13it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 81/67569 [00:07<1:20:14, 14.02it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 83/67569 [00:07<1:27:38, 12.83it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 85/67569 [00:07<1:21:26, 13.81it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 87/67569 [00:07<1:32:42, 12.13it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 89/67569 [00:07<1:25:32, 13.15it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 91/67569 [00:07<1:31:12, 12.33it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 93/67569 [00:07<1:24:26, 13.32it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 95/67569 [00:08<1:21:42, 13.76it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 97/67569 [00:08<1:17:07, 14.58it/s]Training | Epoch: 0 | Loss: 466852704.1850 | :   0%|          | 99/67569 [00:08<1:28:40, 12.68it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 99/67569 [00:08<1:28:40, 12.68it/s] Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 101/67569 [00:08<1:24:07, 13.37it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 103/67569 [00:08<1:23:37, 13.45it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 105/67569 [00:08<1:18:23, 14.34it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 107/67569 [00:09<1:26:25, 13.01it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 109/67569 [00:09<1:20:22, 13.99it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 111/67569 [00:09<1:21:54, 13.73it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 113/67569 [00:09<1:17:14, 14.55it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 115/67569 [00:09<1:23:16, 13.50it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 117/67569 [00:09<1:18:17, 14.36it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 119/67569 [00:09<1:31:16, 12.32it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 121/67569 [00:10<1:23:52, 13.40it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 123/67569 [00:10<1:32:05, 12.21it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 125/67569 [00:10<1:24:28, 13.31it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 127/67569 [00:10<1:23:09, 13.52it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 129/67569 [00:10<1:18:10, 14.38it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 131/67569 [00:10<1:34:43, 11.87it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 133/67569 [00:10<1:26:16, 13.03it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 135/67569 [00:11<1:21:45, 13.75it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 137/67569 [00:11<1:17:19, 14.54it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 139/67569 [00:11<1:24:18, 13.33it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 141/67569 [00:11<1:18:59, 14.23it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 143/67569 [00:11<1:26:02, 13.06it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 145/67569 [00:11<1:20:07, 14.03it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 147/67569 [00:11<1:29:41, 12.53it/s]Training | Epoch: 0 | Loss: 11661766.0762 | :   0%|          | 149/67569 [00:12<1:22:48, 13.57it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 149/67569 [00:12<1:22:48, 13.57it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 151/67569 [00:12<1:29:05, 12.61it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 153/67569 [00:12<1:22:42, 13.59it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 155/67569 [00:12<1:23:40, 13.43it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 157/67569 [00:12<1:18:28, 14.32it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 159/67569 [00:12<1:31:56, 12.22it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 161/67569 [00:13<1:24:16, 13.33it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 163/67569 [00:13<1:36:57, 11.59it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 165/67569 [00:13<1:28:41, 12.67it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 167/67569 [00:13<1:25:03, 13.21it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 169/67569 [00:13<1:19:22, 14.15it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 171/67569 [00:13<1:21:45, 13.74it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 173/67569 [00:13<1:17:06, 14.57it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 175/67569 [00:14<1:32:35, 12.13it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 177/67569 [00:14<1:24:42, 13.26it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 179/67569 [00:14<1:28:29, 12.69it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 181/67569 [00:14<1:22:42, 13.58it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 183/67569 [00:14<1:23:31, 13.45it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 185/67569 [00:14<1:18:21, 14.33it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 187/67569 [00:14<1:23:34, 13.44it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 189/67569 [00:15<1:20:18, 13.98it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 191/67569 [00:15<1:28:37, 12.67it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 193/67569 [00:15<1:22:56, 13.54it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 195/67569 [00:15<1:23:56, 13.38it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 197/67569 [00:15<1:18:34, 14.29it/s]Training | Epoch: 0 | Loss: 16183963.9750 | :   0%|          | 199/67569 [00:15<1:30:08, 12.46it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 199/67569 [00:15<1:30:08, 12.46it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 201/67569 [00:16<1:23:16, 13.48it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 203/67569 [00:16<1:21:25, 13.79it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 205/67569 [00:16<1:17:27, 14.49it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 207/67569 [00:16<1:26:52, 12.92it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 209/67569 [00:16<1:20:49, 13.89it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 211/67569 [00:16<1:34:07, 11.93it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 213/67569 [00:16<1:25:49, 13.08it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 215/67569 [00:17<1:24:19, 13.31it/s]Training | Epoch: 0 | Loss: 12852229.2163 | :   0%|          | 216/67569 [00:17<1:29:08, 12.59it/s]
Traceback (most recent call last):
  File "/net/vast-storage.ib.cluster/scratch/user/valmiki/stats_lecture/src/price_predictor/train.py", line 233, in <module>
    train(config)
  File "/net/vast-storage.ib.cluster/scratch/user/valmiki/stats_lecture/src/price_predictor/train.py", line 182, in train
    train_loss, train_acc = train_epoch(
                            ^^^^^^^^^^^^
  File "/net/vast-storage.ib.cluster/scratch/user/valmiki/stats_lecture/src/price_predictor/train.py", line 35, in train_epoch
    for i, seq in enumerate(bar):
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/tqdm/std.py", line 1181, in __iter__
    for obj in iterable:
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1326, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1372, in _process_data
    data.reraise()
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/_utils.py", line 722, in reraise
    raise exception
RuntimeError: Caught RuntimeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
           ^^^^^^^^^^^^^^^^^^^^
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 277, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 121, in collate
    return collate_fn_map[elem_type](batch, collate_fn_map=collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/om2/user/valmiki/miniconda/envs/pytorch/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 173, in collate_tensor_fn
    out = elem.new(storage).resize_(len(batch), *list(elem.size()))
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to resize storage that is not resizable

wandb: - 0.006 MB of 0.006 MB uploadedwandb: \ 0.016 MB of 0.027 MB uploaded (0.003 MB deduped)wandb: | 0.023 MB of 0.027 MB uploaded (0.003 MB deduped)wandb: 
wandb: Run history:
wandb: running_loss ‚ñà‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb: running_loss 12852229.21625
wandb: 
wandb: üöÄ View run woven-frog-3 at: https://wandb.ai/valmiki-kothare-vk/Price%20Predictor/runs/5qtgc03h
wandb: Ô∏è‚ö° View job at https://wandb.ai/valmiki-kothare-vk/Price%20Predictor/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE2MTAyNzQzOQ==/version_details/v2
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240410_015828-5qtgc03h/logs
